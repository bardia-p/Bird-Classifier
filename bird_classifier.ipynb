{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2GJwhRulmOR"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OZz-wAClh20"
      },
      "outputs": [],
      "source": [
        "# SciKit Learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# TensorFlow\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Vision Transformer\n",
        "\n",
        "from timm.models import vit_base_patch16_224\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Data Analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Audio Feature Extraction\n",
        "import librosa\n",
        "\n",
        "# Image Analysis\n",
        "from PIL import Image\n",
        "\n",
        "# General\n",
        "from google.colab import drive\n",
        "import os\n",
        "import zipfile\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "import re\n",
        "from io import BytesIO, TextIOWrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IoG-r2slvSP"
      },
      "source": [
        "# Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HADM8vWlxEn"
      },
      "outputs": [],
      "source": [
        "# Data paths\n",
        "audio_zip_folder_2021 = \"2021(audio).zip\"\n",
        "spec_zip_folder_2021 = \"2021(spec).zip\"\n",
        "audio_zip_folder_2022 = \"2022(audio).zip\"\n",
        "spec_zip_folder_2022 = \"2022(spec).zip\"\n",
        "# These are our assumptions\n",
        "top_audio_folder_2021 = \"2021(audio)/\"\n",
        "top_spec_folder_2021 = \"2021(spec)/\"\n",
        "top_audio_folder_2022 = \"2022(audio)/\"\n",
        "top_spec_folder_2022 = \"2022(spec)/\"\n",
        "\n",
        "# Folders\n",
        "root_folder = \"/content/\"\n",
        "drive_folder = root_folder + \"drive/MyDrive/\"\n",
        "audio_folder_2021 = root_folder + \"2021_audio/\"\n",
        "spec_folder_2021 = root_folder + \"2021_spec/\"\n",
        "audio_folder_2022 = root_folder + \"2022_audio/\"\n",
        "spec_folder_2022 = root_folder + \"2022_spec/\"\n",
        "\n",
        "# Test paths\n",
        "audio_zip_folder_2023 = \"2023(audio).zip\"\n",
        "spec_zip_folder_2023 = \"2023(spec).zip\"\n",
        "top_audio_folder_2023 = \"2023(audio)/\"\n",
        "top_spec_folder_2023 = \"2023(spec)/\"\n",
        "audio_folder_2023 = root_folder + \"2023_audio/\"\n",
        "spec_folder_2023 = root_folder + \"2023_spec/\"\n",
        "\n",
        "# Constants\n",
        "perform_cross_validation = False\n",
        "train_and_save_classifiers = False\n",
        "\n",
        "perform_train = perform_cross_validation or train_and_save_classifiers\n",
        "perform_test = True\n",
        "\n",
        "fnn_model_name = \"fnn.keras\"\n",
        "transformer_model_name = \"transformer.pth\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yxyf3NjllTW"
      },
      "source": [
        "# Preparing the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Znz5kDU6nWDf"
      },
      "source": [
        "## Unzipping the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rRLjTBAl1lw",
        "outputId": "bf450bbc-a294-4aa3-ca32-b289571df56f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "if perform_train:\n",
        "    # Unzipping 2021 data\n",
        "    if not os.path.isdir(audio_folder_2021):\n",
        "        with zipfile.ZipFile(drive_folder + audio_zip_folder_2021, 'r') as zip_ref:\n",
        "            zip_ref.extractall(audio_folder_2021)\n",
        "\n",
        "    if not os.path.isdir(spec_folder_2021):\n",
        "        with zipfile.ZipFile(drive_folder + spec_zip_folder_2021, 'r') as zip_ref:\n",
        "            zip_ref.extractall(spec_folder_2021)\n",
        "\n",
        "    # Unzipping 2022 data\n",
        "    if not os.path.isdir(audio_folder_2022):\n",
        "        with zipfile.ZipFile(drive_folder + audio_zip_folder_2022, 'r') as zip_ref:\n",
        "            zip_ref.extractall(audio_folder_2022)\n",
        "\n",
        "    if not os.path.isdir(spec_folder_2022):\n",
        "        with zipfile.ZipFile(drive_folder + spec_zip_folder_2022, 'r') as zip_ref:\n",
        "            zip_ref.extractall(spec_folder_2022)\n",
        "\n",
        "if perform_test:\n",
        "    # Unzipping the test data\n",
        "    if not os.path.isdir(audio_folder_2023):\n",
        "        with zipfile.ZipFile(drive_folder + audio_zip_folder_2023, 'r') as zip_ref:\n",
        "            zip_ref.extractall(audio_folder_2023)\n",
        "\n",
        "    if not os.path.isdir(spec_folder_2023):\n",
        "        with zipfile.ZipFile(drive_folder + spec_zip_folder_2023, 'r') as zip_ref:\n",
        "            zip_ref.extractall(spec_folder_2023)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzF3DB7wr1so"
      },
      "source": [
        "## Creating the DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EDa25T4r4ZE",
        "outputId": "7d5ddaa2-052f-494c-8a0f-a7451103bcd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                      fname  AMRO  BHCO  CHSW  EUST  GRCA  HOSP  HOWR  NOCA  \\\n",
            "0  PEN0_20210609_055000.mp3     1     0     1     0     0     1     0     0   \n",
            "1  PEN0_20210609_060000.mp3     0     1     1     0     0     1     1     0   \n",
            "2  PEN0_20210609_061000.mp3     1     0     1     0     0     0     0     0   \n",
            "3  PEN0_20210609_062000.mp3     0     1     1     1     0     1     0     0   \n",
            "4  PEN0_20210609_063000.mp3     0     0     1     0     0     1     0     1   \n",
            "\n",
            "   RBGU  RWBL                                         audio_path  \\\n",
            "0     0     0  /content/2021_audio/2021(audio)/PEN/PEN0_20210...   \n",
            "1     0     0  /content/2021_audio/2021(audio)/PEN/PEN0_20210...   \n",
            "2     0     0  /content/2021_audio/2021(audio)/PEN/PEN0_20210...   \n",
            "3     0     0  /content/2021_audio/2021(audio)/PEN/PEN0_20210...   \n",
            "4     0     0  /content/2021_audio/2021(audio)/PEN/PEN0_20210...   \n",
            "\n",
            "                                           spec_path  \n",
            "0  /content/2021_spec/2021(spec)/PEN/PEN0_2021060...  \n",
            "1  /content/2021_spec/2021(spec)/PEN/PEN0_2021060...  \n",
            "2  /content/2021_spec/2021(spec)/PEN/PEN0_2021060...  \n",
            "3  /content/2021_spec/2021(spec)/PEN/PEN0_2021060...  \n",
            "4  /content/2021_spec/2021(spec)/PEN/PEN0_2021060...  \n"
          ]
        }
      ],
      "source": [
        "def load_data_for_year(audio_folder, spec_folder, top_audio_folder, top_spec_folder):\n",
        "    dataframes = []\n",
        "\n",
        "    data_dir = audio_folder + top_audio_folder\n",
        "\n",
        "    for loc in os.listdir(data_dir):\n",
        "        if loc == \".DS_Store\" or loc == \"__MACOSX\":\n",
        "            continue\n",
        "\n",
        "        train_dir = data_dir + \"/\" + loc\n",
        "\n",
        "        audio_dir = audio_folder + top_audio_folder + loc\n",
        "        spec_dir = spec_folder + top_spec_folder + loc\n",
        "\n",
        "        # Loading the train_labels\n",
        "        loc_train_labels = pd.read_csv(train_dir + \"/train_labels.csv\")\n",
        "        loc_dataframe = pd.DataFrame(loc_train_labels)\n",
        "\n",
        "        # Updating the audio paths\n",
        "        loc_dataframe[\"audio_path\"] = audio_dir + \"/\" + loc_dataframe[\"fname\"]\n",
        "\n",
        "        # Updating the spec paths\n",
        "        loc_dataframe[\"spec_path\"] = spec_dir + \"/\" + loc_dataframe[\"fname\"].apply(lambda x: x.strip(\".mp3\") + \".png\")\n",
        "\n",
        "        dataframes.append(loc_dataframe)\n",
        "\n",
        "    # Putting everything together!\n",
        "    return pd.concat(dataframes)\n",
        "\n",
        "# List of label columns\n",
        "label_columns = [\"AMRO\", \"BHCO\", \"CHSW\", \"EUST\", \"GRCA\", \"HOSP\", \"HOWR\", \"NOCA\", \"RBGU\", \"RWBL\"]\n",
        "\n",
        "if perform_train:\n",
        "    data_2021 = load_data_for_year(audio_folder_2021, spec_folder_2021,\n",
        "                                top_audio_folder_2021, top_spec_folder_2021)\n",
        "    data_2022 = load_data_for_year(audio_folder_2022, spec_folder_2022,\n",
        "                                top_audio_folder_2022, top_spec_folder_2022)\n",
        "\n",
        "    data = pd.concat([data_2021, data_2022], axis=0, ignore_index=True)\n",
        "    print(data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAI25jfswB2O"
      },
      "source": [
        "## Removing Blank Rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgBiEmYkwDqC"
      },
      "outputs": [],
      "source": [
        "if perform_train:\n",
        "    # Check for rows with all-zero labels\n",
        "    blank_labels = data[(data[label_columns].sum(axis=1) == 0)]\n",
        "\n",
        "    # Print the rows with blank labels\n",
        "    if not blank_labels.empty:\n",
        "        print(f\"Found {len(blank_labels)} rows with blank labels:\")\n",
        "    else:\n",
        "        print(\"No rows with blank labels found.\")\n",
        "\n",
        "    # Remove rows with all-zero labels\n",
        "    data = data[data[label_columns].sum(axis=1) > 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKMqk6H1D4Eo"
      },
      "source": [
        "## Adding the Location Field"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D68hi9jJD53Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c3271ac-52e4-46dd-f23e-3fe0cd0be357"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'BRY': 0, 'CAL': 1, 'FIO': 2, 'HAR': 3, 'KEA': 4, 'LAW': 5, 'LIF': 6, 'MCK': 7, 'PEN': 8, 'SYL': 9, 'WAT': 10}\n"
          ]
        }
      ],
      "source": [
        "if perform_train:\n",
        "    # Extract the three-letter location code from the audio_path\n",
        "    data['location'] = data['audio_path'].str.extract(r'/(\\w{3})/')\n",
        "\n",
        "    # Label Encode encode the location column\n",
        "    encoder = LabelEncoder()\n",
        "    encoded_fields = encoder.fit_transform(data['location'])\n",
        "    category_mapping = {category: i for i, category in enumerate(encoder.classes_)}\n",
        "    print(category_mapping)\n",
        "    data['location'] = encoder.fit_transform(data['location'])\n",
        "    data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4QIALKZRU0X"
      },
      "source": [
        "## Fixing Discrepencies in the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BT1K-LoGRXR7"
      },
      "outputs": [],
      "source": [
        "if perform_train:\n",
        "    data[label_columns] = data[label_columns].replace(2, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIdjb37fFuqM"
      },
      "source": [
        "## Shuffling the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXASlhm5Ft3W"
      },
      "outputs": [],
      "source": [
        "if perform_train:\n",
        "    data = data.sample(n=len(data)).reset_index(drop=True)\n",
        "    data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXW2XYF6wqsG"
      },
      "source": [
        "# Feature Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NejuPKhN5gJi"
      },
      "source": [
        "## Audio Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqfpJVIZ5Cql"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "SAMPLE_RATE = 16000\n",
        "FRAME_SIZE = 1024  # Typical frame size for audio processing\n",
        "HOP_LENGTH = 512  # Overlap of 50%\n",
        "N_MFCC = 13  # Number of MFCCs to extract\n",
        "N_CHROMA = 12  # Number of chroma features\n",
        "N_CONTRAST = 7  # Number of spectral contrast bands\n",
        "\n",
        "# Preprocessing function for MFCCs, Chroma, and Spectral Contrast\n",
        "def preprocess_audio_features(file_path):\n",
        "    try:\n",
        "        # Load audio file\n",
        "        audio, sr = librosa.load(file_path, sr=SAMPLE_RATE)\n",
        "\n",
        "        # Normalize audio\n",
        "        audio = librosa.util.normalize(audio)\n",
        "\n",
        "        # Extract MFCCs\n",
        "        mfccs = librosa.feature.mfcc(\n",
        "            y=audio, sr=sr, n_mfcc=N_MFCC, n_fft=FRAME_SIZE, hop_length=HOP_LENGTH\n",
        "        )\n",
        "        mfcc_mean = np.mean(mfccs, axis=1)  # Compute mean across time\n",
        "\n",
        "        # Extract Chroma features\n",
        "        chroma = librosa.feature.chroma_stft(\n",
        "            y=audio, sr=sr, n_fft=FRAME_SIZE, hop_length=HOP_LENGTH\n",
        "        )\n",
        "        chroma_mean = np.mean(chroma, axis=1)  # Compute mean across time\n",
        "\n",
        "        # Extract Spectral Contrast\n",
        "        contrast = librosa.feature.spectral_contrast(\n",
        "            y=audio, sr=sr, n_fft=FRAME_SIZE, hop_length=HOP_LENGTH\n",
        "        )\n",
        "        contrast_mean = np.mean(contrast, axis=1)  # Compute mean across time\n",
        "\n",
        "        # Combine all features\n",
        "        combined_features = np.concatenate([mfcc_mean, chroma_mean, contrast_mean])  # Shape: (N_MFCC + N_CHROMA + N_CONTRAST,)\n",
        "\n",
        "        return combined_features\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path}: {e}\")\n",
        "        return np.zeros(N_MFCC + N_CHROMA + N_CONTRAST)  # Return zeros if processing fails\n",
        "\n",
        "# Function to generate feature set\n",
        "def generate_audio_feature_set(data):\n",
        "    audio_features = []\n",
        "    for index, row in data.iterrows():\n",
        "        audio_path = row['audio_path']\n",
        "        features = preprocess_audio_features(audio_path)\n",
        "        audio_features.append(features)  # Append the combined feature vector\n",
        "    data[\"audio_features\"] = audio_features\n",
        "    return data\n",
        "\n",
        "if perform_train:\n",
        "    data = generate_audio_feature_set(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lrt5ywMD5iMD"
      },
      "source": [
        "## Spectrogram Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8ViXDKO5qJQ"
      },
      "outputs": [],
      "source": [
        "class SpectrogramDataset(Dataset):\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "\n",
        "        self.dataframe = dataframe\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.dataframe.iloc[idx]['spec_path']\n",
        "        labels = self.dataframe.iloc[idx][1:11].values.astype('float32')\n",
        "\n",
        "        # Load the spectrogram image\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYBS_nygC8xZ"
      },
      "source": [
        "# Creating the Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BC2jn4TyDMuM"
      },
      "source": [
        "## Building the FNN for Audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxEdg1iqC4P2"
      },
      "outputs": [],
      "source": [
        "# Define the Dense model\n",
        "def build_dense_model(input_shape, num_classes):\n",
        "    # Clear session before retraining\n",
        "    K.clear_session()\n",
        "\n",
        "    model = Sequential([\n",
        "        Dense(2048, activation=LeakyReLU(alpha=0.01), kernel_regularizer=regularizers.L2(0.2), input_shape=(input_shape,)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(2048, activation=LeakyReLU(alpha=0.01), kernel_regularizer=regularizers.L2(0.2)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(1024, activation=LeakyReLU(alpha=0.01), kernel_regularizer=regularizers.L2(0.2)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(1024, activation=LeakyReLU(alpha=0.01), kernel_regularizer=regularizers.L2(0.2)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(1024, activation=LeakyReLU(alpha=0.01), kernel_regularizer=regularizers.L2(0.2)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(num_classes, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(\n",
        "        optimizer=SGD(learning_rate=1.0, momentum=0.9),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy', Precision(), Recall()]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Callbacks for training\n",
        "reduce_lr_fnn = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fH2J4-cK6jV"
      },
      "source": [
        "## Building the Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBrgsBpTK9Pc"
      },
      "outputs": [],
      "source": [
        "class ViTClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(ViTClassifier, self).__init__()\n",
        "        # Load pre-trained ViT\n",
        "        self.vit = vit_base_patch16_224(pretrained=True)\n",
        "        # Modify the classifier head\n",
        "        self.vit.head = nn.Linear(self.vit.head.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.vit(x)\n",
        "\n",
        "# Update train_one_epoch and evaluate to return precision and recall\n",
        "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for inputs, labels in tqdm(dataloader, desc=\"Training\", leave=False):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        preds = torch.sigmoid(outputs)\n",
        "        all_preds.extend(preds.detach().cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    all_preds = (torch.tensor(all_preds) >= 0.5).int().numpy()\n",
        "    precision = precision_score(all_labels, all_preds, average=\"micro\")\n",
        "    recall = recall_score(all_labels, all_preds, average=\"micro\")\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average=\"micro\")\n",
        "    return total_loss / len(dataloader), accuracy, precision, recall, f1\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device, split_name=\"Validation\"):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(dataloader, desc=f\"{split_name}\", leave=False):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels.float())\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            preds = torch.sigmoid(outputs)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    all_preds = (torch.tensor(all_preds) >= 0.5).int().numpy()\n",
        "    precision = precision_score(all_labels, all_preds, average=\"micro\")\n",
        "    recall = recall_score(all_labels, all_preds, average=\"micro\")\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average=\"micro\")\n",
        "    return all_preds, total_loss / len(dataloader), accuracy, precision, recall, f1\n",
        "\n",
        "def train_and_evaluate(model, train_dataloader, val_dataloader, test_dataloader, num_epochs, device, save_model=False):\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = Adam(model.parameters(), lr=1e-4, weight_decay=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.7)\n",
        "\n",
        "    best_val_loss = float(\"inf\")\n",
        "    patience = 10\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "    # Save metrics for plotting\n",
        "    metrics = {\n",
        "        \"train_loss\": [],\n",
        "        \"val_loss\": [],\n",
        "        \"train_accuracy\": [],\n",
        "        \"val_accuracy\": [],\n",
        "        \"train_precision\": [],\n",
        "        \"val_precision\": [],\n",
        "        \"train_recall\": [],\n",
        "        \"val_recall\": [],\n",
        "        \"train_f1\": [],\n",
        "        \"val_f1\": [],\n",
        "    }\n",
        "\n",
        "    save_path = f\"{transformer_model_name}\"\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "        # Training\n",
        "        train_loss, train_accuracy, train_precision, train_recall, train_f1 = train_one_epoch(\n",
        "            model, train_dataloader, criterion, optimizer, device\n",
        "        )\n",
        "\n",
        "        # Validation\n",
        "        preds, val_loss, val_accuracy, val_precision, val_recall, val_f1 = evaluate(\n",
        "            model, val_dataloader, criterion, device, split_name=\"Validation\"\n",
        "        )\n",
        "\n",
        "        # Store metrics\n",
        "        metrics[\"train_loss\"].append(train_loss)\n",
        "        metrics[\"val_loss\"].append(val_loss)\n",
        "        metrics[\"train_accuracy\"].append(train_accuracy)\n",
        "        metrics[\"val_accuracy\"].append(val_accuracy)\n",
        "        metrics[\"train_precision\"].append(train_precision)\n",
        "        metrics[\"val_precision\"].append(val_precision)\n",
        "        metrics[\"train_recall\"].append(train_recall)\n",
        "        metrics[\"val_recall\"].append(val_recall)\n",
        "        metrics[\"train_f1\"].append(train_f1)\n",
        "        metrics[\"val_f1\"].append(val_f1)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        print(f\"  Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1: {train_f1:.4f}\")\n",
        "        print(f\"  Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1: {val_f1:.4f}\")\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            epochs_without_improvement = 0\n",
        "            if save_model:\n",
        "                torch.save(model, save_path)\n",
        "                print(f\"  Best model saved to {save_path}.\")\n",
        "\n",
        "    if type(test_dataloader) != type(None):\n",
        "        print(\"\\nEvaluating on the Test Set...\")\n",
        "        preds, test_loss, test_accuracy, test_precision, test_recall, test_f1 = evaluate(\n",
        "            model, test_dataloader, criterion, device, split_name=\"Test\"\n",
        "        )\n",
        "        print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}, Test F1: {test_f1:.4f}\")\n",
        "\n",
        "        return preds, metrics\n",
        "    return [], metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVB-AsJzFhnP"
      },
      "source": [
        "# Training the Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZTjobR9Fmzg"
      },
      "source": [
        "## Running the Audio Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KimyDdFmFmOE"
      },
      "outputs": [],
      "source": [
        "def run_fnn_model(train, train_labels, validation, validation_labels, test, num_epochs, save_model=False):\n",
        "    # Input shape and number of classes\n",
        "    audio_features_train = [arr.tolist() for arr in train[\"audio_features\"]]\n",
        "    audio_features_validation = [arr.tolist() for arr in validation[\"audio_features\"]]\n",
        "\n",
        "    # Add location features to audio features\n",
        "    train_combined_features = np.hstack((np.array(audio_features_train), train[\"location\"].to_numpy().reshape(-1, 1)))\n",
        "    validation_combined_features = np.hstack((np.array(audio_features_validation), validation[\"location\"].to_numpy().reshape(-1, 1)))\n",
        "\n",
        "    input_shape = train_combined_features.shape[1]  # Number of MFCC features (e.g., 13)\n",
        "    num_classes = train_labels.shape[1]  # Number of bird species (e.g., 10)\n",
        "\n",
        "    # Build the model\n",
        "    fnn_model = build_dense_model(input_shape, num_classes)\n",
        "    fnn_model.summary()\n",
        "\n",
        "    # Train the model\n",
        "    fnn_history = fnn_model.fit(\n",
        "        train_combined_features, train_labels,\n",
        "        validation_data=(validation_combined_features, validation_labels),  # Include validation data\n",
        "        batch_size=32,\n",
        "        epochs=num_epochs,\n",
        "        callbacks=[reduce_lr_fnn],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    if save_model:\n",
        "        fnn_model.save(fnn_model_name)\n",
        "\n",
        "    if type(test) != type(None):\n",
        "        audio_features_test = [arr.tolist() for arr in test[\"audio_features\"]]\n",
        "        test_combined_features = np.hstack((np.array(audio_features_test), test[\"location\"].to_numpy().reshape(-1, 1)))\n",
        "\n",
        "        # Run on test data\n",
        "        fnn_preds = fnn_model.predict(test_combined_features)\n",
        "\n",
        "        return fnn_preds, fnn_history\n",
        "    return [], fnn_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvtPzHCFORLe"
      },
      "source": [
        "## Running the Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-myrMMpOTbr"
      },
      "outputs": [],
      "source": [
        "def run_transformer_model(train, train_labels, validation, validation_labels, test, num_epochs, save_model=False):\n",
        "    transformer_model = ViTClassifier(num_classes=len(label_columns))\n",
        "    # Set up device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    transformer_model.to(device)\n",
        "\n",
        "    # Create datasets for training, validation, and testing\n",
        "    train_dataset = SpectrogramDataset(dataframe=train, transform=transform)\n",
        "    val_dataset = SpectrogramDataset(dataframe=validation, transform=transform)\n",
        "\n",
        "    # Create DataLoaders for training, validation, and testing\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    if type(test) != type(None):\n",
        "        test_dataset = SpectrogramDataset(dataframe=test, transform=transform)\n",
        "        test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "    else:\n",
        "        test_dataloader = None\n",
        "\n",
        "    # Train and test\n",
        "    transformer_preds, transformer_history = train_and_evaluate(\n",
        "        model=transformer_model,\n",
        "        train_dataloader=train_dataloader,\n",
        "        val_dataloader=val_dataloader,\n",
        "        test_dataloader=test_dataloader,\n",
        "        num_epochs=num_epochs,\n",
        "        device=device,\n",
        "        save_model=save_model\n",
        "    )\n",
        "\n",
        "    return transformer_preds, transformer_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SfQ1FKV6Vrf"
      },
      "source": [
        "## Visualizing the Performance Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_enIn-c6YdW"
      },
      "outputs": [],
      "source": [
        "def plot_metrics(fnn_history, transformer_history):\n",
        "    fig, axs = plt.subplots(2, 4, figsize=(20, 8))\n",
        "\n",
        "    # Plotting the FNN metrics\n",
        "    axs[0, 0].plot(fnn_history.history['accuracy'], label='Training Accuracy', color='blue')\n",
        "    axs[0, 0].plot(fnn_history.history['val_accuracy'], label='Validation Accuracy', color='orange')\n",
        "    axs[0, 0].set_title(\"FNN Accuracy\")\n",
        "    axs[0, 0].set_xlabel('Epochs')\n",
        "    axs[0, 0].set_ylabel('Accuracy')\n",
        "    axs[0, 0].legend()\n",
        "\n",
        "    axs[0, 1].plot(fnn_history.history['loss'], label='Training Loss', color='blue')\n",
        "    axs[0, 1].plot(fnn_history.history['val_loss'], label='Validation Loss', color='orange')\n",
        "    axs[0, 1].set_title(\"FNN Loss\")\n",
        "    axs[0, 1].set_xlabel('Epochs')\n",
        "    axs[0, 1].set_ylabel('Loss')\n",
        "    axs[0, 1].legend()\n",
        "\n",
        "    axs[0, 2].plot(fnn_history.history['precision'], label='Training Precision', color='blue')\n",
        "    axs[0, 2].plot(fnn_history.history['val_precision'], label='Validation Precision', color='orange')\n",
        "    axs[0, 2].set_title(\"FNN Precision\")\n",
        "    axs[0, 2].set_xlabel('Epochs')\n",
        "    axs[0, 2].set_ylabel('Precision')\n",
        "    axs[0, 2].legend()\n",
        "\n",
        "    axs[0, 3].plot(fnn_history.history['recall'], label='Training Recall', color='blue')\n",
        "    axs[0, 3].plot(fnn_history.history['val_recall'], label='Validation Recall', color='orange')\n",
        "    axs[0, 3].set_title(\"FNN Recall\")\n",
        "    axs[0, 3].set_xlabel('Epochs')\n",
        "    axs[0, 3].set_ylabel('Recall')\n",
        "    axs[0, 3].legend()\n",
        "\n",
        "\n",
        "    # Plotting the Transformer metrics\n",
        "    epochs = range(1, len(transformer_history[\"train_loss\"]) + 1)\n",
        "\n",
        "    axs[1, 0].plot(epochs, transformer_history['train_accuracy'], label='Training Accuracy', color='blue')\n",
        "    axs[1, 0].plot(epochs, transformer_history['val_accuracy'], label='Validation Accuracy', color='orange')\n",
        "    axs[1, 0].set_title(\"Transformer Accuracy\")\n",
        "    axs[1, 0].set_xlabel('Epochs')\n",
        "    axs[1, 0].set_ylabel('Accuracy')\n",
        "    axs[1, 0].legend()\n",
        "\n",
        "    axs[1, 1].plot(epochs, transformer_history['train_loss'], label='Training Loss', color='blue')\n",
        "    axs[1, 1].plot(epochs, transformer_history['val_loss'], label='Validation Loss', color='orange')\n",
        "    axs[1, 1].set_title(\"Transformer Loss\")\n",
        "    axs[1, 1].set_xlabel('Epochs')\n",
        "    axs[1, 1].set_ylabel('Loss')\n",
        "    axs[1, 1].legend()\n",
        "\n",
        "    axs[1, 2].plot(epochs, transformer_history['train_precision'], label='Training Precision', color='blue')\n",
        "    axs[1, 2].plot(epochs, transformer_history['val_precision'], label='Validation Precision', color='orange')\n",
        "    axs[1, 2].set_title('Transformer Precision')\n",
        "    axs[1, 2].set_xlabel('Epochs')\n",
        "    axs[1, 2].set_ylabel('Precision')\n",
        "    axs[1, 2].legend()\n",
        "\n",
        "    axs[1, 3].plot(epochs, transformer_history['train_recall'], label='Training Recall', color='blue')\n",
        "    axs[1, 3].plot(epochs, transformer_history['val_recall'], label='Validation Recall', color='orange')\n",
        "    axs[1, 3].set_title('Transformer Recall')\n",
        "    axs[1, 3].set_xlabel('Epochs')\n",
        "    axs[1, 3].set_ylabel('Recall')\n",
        "    axs[1, 3].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xBsFMUKQ5oc"
      },
      "source": [
        "## Performing K-Fold Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MbTORtxQ-VY"
      },
      "outputs": [],
      "source": [
        "if perform_cross_validation:\n",
        "    f1_scores = []\n",
        "    selected_thresolds = []\n",
        "    n_folds = 5\n",
        "    fnn_epochs = 100\n",
        "    transformer_epochs = 20\n",
        "\n",
        "    for i in range(n_folds):\n",
        "        # Split the data to train, validation and test\n",
        "        train, test = train_test_split(data, test_size=0.15)\n",
        "        train, validation = train_test_split(train, test_size=0.15)\n",
        "\n",
        "        train_labels = train[label_columns]\n",
        "        validation_labels = validation[label_columns]\n",
        "        test_labels = test[label_columns]\n",
        "\n",
        "        # Run both models\n",
        "        fnn_preds, fnn_history = run_fnn_model(train, train_labels, validation, validation_labels, test, fnn_epochs)\n",
        "        transformer_preds, transformer_history = run_transformer_model(train, train_labels, validation, validation_labels, test, transformer_epochs)\n",
        "\n",
        "        # Combine their probabilities\n",
        "        final_preds = (fnn_preds + transformer_preds) / 2\n",
        "\n",
        "        # Calculate F1\n",
        "        thresholds = np.linspace(0, 1, 101)\n",
        "        best_threshold = 0\n",
        "        best_f1 = 0\n",
        "        for threshold in thresholds:\n",
        "            pred_labels_binary = (final_preds >= threshold).astype(int)\n",
        "\n",
        "            f1 = f1_score(test_labels, pred_labels_binary, average='micro')\n",
        "            if f1 > best_f1:\n",
        "                best_f1 = f1\n",
        "                best_threshold = threshold\n",
        "\n",
        "        print(f\"F1: {best_f1} at threshold: {best_threshold}\")\n",
        "\n",
        "        f1_scores.append(best_f1)\n",
        "        selected_thresolds.append(best_threshold)\n",
        "\n",
        "        # Plotting the metrics\n",
        "        plot_metrics(fnn_history, transformer_history)\n",
        "\n",
        "    print(f\"F1 mean: {np.mean(f1_scores)} and std-dev: {np.std(f1_scores)}\")\n",
        "    print(f\"Threshold mean: {np.mean(selected_thresolds)} and std-dev: {np.std(selected_thresolds)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wr2oEE54XdP8"
      },
      "source": [
        "## Training the Models on All the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Spqa9v8Xqjk"
      },
      "outputs": [],
      "source": [
        "if train_and_save_classifiers:\n",
        "    train, validation = train_test_split(data, test_size=0.15)\n",
        "    train_labels = train[label_columns]\n",
        "    validation_labels = validation[label_columns]\n",
        "\n",
        "    epochs_fnn = 200\n",
        "    epochs_transformer = 40\n",
        "\n",
        "    _, fnn_history = run_fnn_model(train, train_labels, validation, validation_labels, None, epochs_fnn, save_model=True)\n",
        "    shutil.copy(root_folder + fnn_model_name, drive_folder + fnn_model_name)\n",
        "    _, transformer_history = run_transformer_model(train, train_labels, validation, validation_labels, None, epochs_transformer, save_model=True)\n",
        "\n",
        "    # Plotting the metrics\n",
        "    plot_metrics(fnn_history, transformer_history)\n",
        "\n",
        "    #shutil.copy(root_folder + fnn_model_name, drive_folder + fnn_model_name)\n",
        "    shutil.copy(root_folder + transformer_model_name, drive_folder + transformer_model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBp_SAF3KIpv"
      },
      "source": [
        "# Running the Data on Blind Test Data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the Test Data Frames"
      ],
      "metadata": {
        "id": "3orZFmMRCAdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_zip = \"2023_predictions.zip\"\n",
        "predictions_folder = \"2023_predictions/\"\n",
        "\n",
        "if not os.path.isdir(predictions_folder):\n",
        "    with zipfile.ZipFile(root_folder + predictions_zip, 'r') as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "\n",
        "\n",
        "def load_test_data():\n",
        "    dataframes = []\n",
        "\n",
        "    data_dir = root_folder + predictions_folder\n",
        "\n",
        "    # Obtained from a label encoder\n",
        "    location_encodings = {'BRY': 0, 'CAL': 1, 'FIO': 2, 'HAR': 3, 'KEA': 4,\n",
        "                          'LAW': 5, 'LIF': 6, 'MCK': 7, 'PEN': 8, 'SYL': 9,\n",
        "                          'WAT': 10}\n",
        "\n",
        "    for loc in os.listdir(data_dir):\n",
        "        if loc == \".DS_Store\" or loc == \"__MACOSX\":\n",
        "            continue\n",
        "\n",
        "        loc_dir = data_dir + \"/\" + loc\n",
        "\n",
        "        audio_dir = audio_folder_2023 + top_audio_folder_2023 + loc\n",
        "        spec_dir = spec_folder_2023 + top_spec_folder_2023 + loc\n",
        "\n",
        "        # Loading the predictions\n",
        "        test_labels = pd.read_csv(loc_dir + \"/test_labels.csv\")\n",
        "        predictions_dataframe = pd.DataFrame(test_labels)\n",
        "\n",
        "        # Updating the audio paths\n",
        "        predictions_dataframe[\"audio_path\"] = audio_dir + \"/\" + predictions_dataframe[\"fname\"]\n",
        "\n",
        "        # Updating the spec paths\n",
        "        predictions_dataframe[\"spec_path\"] = spec_dir + \"/\" + predictions_dataframe[\"fname\"].apply(lambda x: x.strip(\".mp3\") + \".png\")\n",
        "\n",
        "        # Add the location encodings\n",
        "        predictions_dataframe['location'] = location_encodings[loc]\n",
        "\n",
        "        dataframes.append(predictions_dataframe)\n",
        "\n",
        "    # Putting everything together!\n",
        "    return dataframes\n",
        "\n",
        "blind_test_data = load_test_data()\n",
        "blind_test_data[0].head()"
      ],
      "metadata": {
        "id": "vDRqIM85CvQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perform Audio Feature Extraction"
      ],
      "metadata": {
        "id": "Aw4t3MqWOZKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_test_data = []\n",
        "\n",
        "for loc_df in blind_test_data:\n",
        "    new_test_data.append(generate_audio_feature_set(loc_df))\n",
        "\n",
        "blind_test_data = new_test_data"
      ],
      "metadata": {
        "id": "93BUQ2_rOd3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the Data for the Models"
      ],
      "metadata": {
        "id": "HShz3LVYWvWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_fnn_input(test):\n",
        "    audio_features_test = [arr.tolist() for arr in test[\"audio_features\"]]\n",
        "    test_combined_features = np.hstack((np.array(audio_features_test), test[\"location\"].to_numpy().reshape(-1, 1)))\n",
        "\n",
        "    return test_combined_features\n",
        "\n",
        "def create_transformer_input(test):\n",
        "    test_dataset = SpectrogramDataset(dataframe=test, transform=transform)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    return test_dataloader"
      ],
      "metadata": {
        "id": "F0gl2gZzWyyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the Models"
      ],
      "metadata": {
        "id": "JfRvAdEOMEuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a model saved in the SavedModel format\n",
        "fnn_model = load_model(root_folder + fnn_model_name, custom_objects={'LeakyReLU': LeakyReLU})\n",
        "\n",
        "# Verify the loaded model\n",
        "fnn_model.summary()\n",
        "\n",
        "transformer_model = torch.load(root_folder + transformer_model_name)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "transformer_model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "b5nU-1SiMGl6",
        "outputId": "d2f15480-b83b-464e-ef20-590b942822f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)                │          \u001b[38;5;34m69,632\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)                │           \u001b[38;5;34m8,192\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)                │       \u001b[38;5;34m4,196,352\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_1                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)                │           \u001b[38;5;34m8,192\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                │       \u001b[38;5;34m2,098,176\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_2                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                │           \u001b[38;5;34m4,096\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                │       \u001b[38;5;34m1,049,600\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_3                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                │           \u001b[38;5;34m4,096\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                │       \u001b[38;5;34m1,049,600\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_4                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                │           \u001b[38;5;34m4,096\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │          \u001b[38;5;34m10,250\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)                │          <span style=\"color: #00af00; text-decoration-color: #00af00\">69,632</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)                │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,192</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)                │       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,196,352</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_1                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)                │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,192</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,098,176</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_2                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,049,600</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_3                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,049,600</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_4                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">10,250</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m16,990,230\u001b[0m (64.81 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">16,990,230</span> (64.81 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,487,946\u001b[0m (32.38 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,487,946</span> (32.38 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m14,336\u001b[0m (56.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,336</span> (56.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m8,487,948\u001b[0m (32.38 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,487,948</span> (32.38 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-58-b564d8197632>:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  transformer_model = torch.load(root_folder + transformer_model_name)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViTClassifier(\n",
              "  (vit): VisionTransformer(\n",
              "    (patch_embed): PatchEmbed(\n",
              "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "      (norm): Identity()\n",
              "    )\n",
              "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
              "    (patch_drop): Identity()\n",
              "    (norm_pre): Identity()\n",
              "    (blocks): Sequential(\n",
              "      (0): Block(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (q_norm): Identity()\n",
              "          (k_norm): Identity()\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (act): GELU(approximate='none')\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (norm): Identity()\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (1): Block(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (q_norm): Identity()\n",
              "          (k_norm): Identity()\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (act): GELU(approximate='none')\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (norm): Identity()\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (2): Block(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (q_norm): Identity()\n",
              "          (k_norm): Identity()\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (act): GELU(approximate='none')\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (norm): Identity()\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (3): Block(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (q_norm): Identity()\n",
              "          (k_norm): Identity()\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (act): GELU(approximate='none')\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (norm): Identity()\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (4): Block(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (q_norm): Identity()\n",
              "          (k_norm): Identity()\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (act): GELU(approximate='none')\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (norm): Identity()\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (5): Block(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (q_norm): Identity()\n",
              "          (k_norm): Identity()\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (act): GELU(approximate='none')\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (norm): Identity()\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (6): Block(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (q_norm): Identity()\n",
              "          (k_norm): Identity()\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (act): GELU(approximate='none')\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (norm): Identity()\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (7): Block(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (q_norm): Identity()\n",
              "          (k_norm): Identity()\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (act): GELU(approximate='none')\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (norm): Identity()\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (8): Block(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (q_norm): Identity()\n",
              "          (k_norm): Identity()\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (act): GELU(approximate='none')\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (norm): Identity()\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (9): Block(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (q_norm): Identity()\n",
              "          (k_norm): Identity()\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (act): GELU(approximate='none')\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (norm): Identity()\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (10): Block(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (q_norm): Identity()\n",
              "          (k_norm): Identity()\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (act): GELU(approximate='none')\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (norm): Identity()\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (11): Block(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (q_norm): Identity()\n",
              "          (k_norm): Identity()\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (act): GELU(approximate='none')\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (norm): Identity()\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "    (fc_norm): Identity()\n",
              "    (head_drop): Dropout(p=0.0, inplace=False)\n",
              "    (head): Linear(in_features=768, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running the Models on Test Data"
      ],
      "metadata": {
        "id": "ey1VhmSMXSz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run on test data\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "ideal_threshold = 0.15\n",
        "\n",
        "predictions = []\n",
        "for loc_df in blind_test_data:\n",
        "    loc_name = \"\".join(set(loc_df['audio_path'].str.extract(r'/(\\w{3})/')[0]))\n",
        "\n",
        "    print(f\"Running the FNN model for location: {loc_name}\")\n",
        "    fnn_preds = fnn_model.predict(create_fnn_input(loc_df))\n",
        "\n",
        "    transformer_preds = []\n",
        "    with torch.no_grad():\n",
        "        print(f\"Running the Transformer model for location: {loc_name}\")\n",
        "        for inputs, labels in tqdm(create_transformer_input(loc_df), desc=f\"Transformer Testing {loc_name}\", leave=False):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = transformer_model(inputs)\n",
        "            preds = torch.sigmoid(outputs)\n",
        "            transformer_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "    transformer_preds = (torch.tensor(transformer_preds) >= 0.5).int().numpy()\n",
        "\n",
        "    # Combine the preds\n",
        "    final_preds = (fnn_preds + transformer_preds) / 2\n",
        "\n",
        "    pred_labels_binary = (final_preds >= ideal_threshold).astype(int)\n",
        "\n",
        "    pred_df = loc_df.iloc[:, 0:len(label_columns)+1]\n",
        "    pred_df[label_columns] = pred_labels_binary\n",
        "\n",
        "    predictions.append(pred_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XO2FDHKKXUuI",
        "outputId": "56489fa5-36ff-441a-d852-c9c6c37b33e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running the FNN model for location: PEN\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "Running the Transformer model for location: PEN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                      "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running the FNN model for location: LIF\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running the Transformer model for location: LIF\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                      "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running the FNN model for location: MCK\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running the Transformer model for location: MCK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                      "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running the FNN model for location: CAL\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running the Transformer model for location: CAL\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                      "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running the FNN model for location: FIO\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running the Transformer model for location: FIO\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                      "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running the FNN model for location: SYL\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running the Transformer model for location: SYL\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                      "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running the FNN model for location: LAW\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running the Transformer model for location: LAW\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                      "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running the FNN model for location: HAR\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running the Transformer model for location: HAR\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                      "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running the FNN model for location: BRY\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running the Transformer model for location: BRY\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                      "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running the FNN model for location: KEA\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running the Transformer model for location: KEA\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                      "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running the FNN model for location: WAT\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running the Transformer model for location: WAT\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the Predictions to CSVs"
      ],
      "metadata": {
        "id": "QnkFIy11eby6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for loc_df in predictions:\n",
        "    loc_name = loc_df['fname'][0][:3]\n",
        "    loc_df.to_csv(root_folder + predictions_folder + loc_name + \"/test_labels.csv\", index=False)"
      ],
      "metadata": {
        "id": "55dn2cFQeduu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zip the Predictions"
      ],
      "metadata": {
        "id": "EoiwZOaehpxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "submission_name = \"group_03\"\n",
        "shutil.make_archive(root_folder + submission_name, 'zip', root_folder, predictions_folder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_FXdNwazhrH2",
        "outputId": "1adc0d8d-cda2-41a9-d0dd-8d6e0642505f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/group_03.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validate the Zip File"
      ],
      "metadata": {
        "id": "kg0GQZ-qeeKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_zip_file(zip_file_path):\n",
        "    \"\"\"\n",
        "    Validate the input zip file against the specified requirements and generate a summary report.\n",
        "\n",
        "    :param zip_file_path: Path to the input zip file.\n",
        "    \"\"\"\n",
        "    report = []\n",
        "\n",
        "    # Check if the file exists\n",
        "    if not os.path.exists(zip_file_path):\n",
        "        return [f\"❌ The file '{zip_file_path}' does not exist. Please provide a valid file path.\"]\n",
        "\n",
        "    # Check 1: Validate the zip file name\n",
        "    if not re.match(r\"group_\\d{2}\\.zip$\", os.path.basename(zip_file_path)):\n",
        "        report.append(\"❌ The zip file name must be in the format 'group_##.zip' (e.g., 'group_01.zip').\")\n",
        "    else:\n",
        "        report.append(\"✅ Zip file name format is valid.\")\n",
        "\n",
        "    # Check if it is a valid zip file\n",
        "    if not zipfile.is_zipfile(zip_file_path):\n",
        "        report.append(\"❌ The file is not a valid zip file.\")\n",
        "        return report\n",
        "\n",
        "    # Required structure and header\n",
        "    required_locations = [\"BRY\", \"CAL\", \"FIO\", \"HAR\", \"KEA\", \"LAW\", \"LIF\", \"MCK\", \"PEN\", \"SYL\", \"WAT\"]\n",
        "    required_header = [\"fname\", \"AMRO\", \"BHCO\", \"CHSW\", \"EUST\", \"GRCA\", \"HOSP\", \"HOWR\", \"NOCA\", \"RBGU\", \"RWBL\"]\n",
        "\n",
        "    # Validate contents of the zip file\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        # Get the list of files in the zip archive\n",
        "        zip_file_list = zip_ref.namelist()\n",
        "\n",
        "        # Check 2: Validate top-level directory\n",
        "        if not any(name.startswith(\"2023_predictions/\") for name in zip_file_list):\n",
        "            report.append(\"❌ The zip file must contain a top-level directory named '2023_predictions'.\")\n",
        "            return report  # Further checks depend on this directory existing\n",
        "\n",
        "        report.append(\"✅ Top-level directory '2023' exists.\")\n",
        "\n",
        "        # Check 3: Validate subdirectories\n",
        "        found_locations = set(\n",
        "            name.split('/')[1]\n",
        "            for name in zip_file_list\n",
        "            if name.startswith(\"2023_predictions/\") and len(name.split('/')) > 1\n",
        "        )\n",
        "        missing_subdirs = set(required_locations) - found_locations\n",
        "        if missing_subdirs:\n",
        "            report.append(f\"❌ Missing subdirectories: {', '.join(missing_subdirs)}.\")\n",
        "        else:\n",
        "            report.append(\"✅ All required subdirectories are present.\")\n",
        "\n",
        "        # Check 4: Validate files in subdirectories\n",
        "        for location in required_locations:\n",
        "            location_files = [\n",
        "                name\n",
        "                for name in zip_file_list\n",
        "                if name.startswith(f\"2023_predictions/{location}/\") and len(name.split('/')) == 3\n",
        "            ]\n",
        "\n",
        "            if not location_files:\n",
        "                report.append(f\"❌ Subdirectory '{location}' does not contain any files.\")\n",
        "                continue\n",
        "\n",
        "            test_labels_files = [f for f in location_files if f.endswith(\"test_labels.csv\")]\n",
        "            if not test_labels_files:\n",
        "                report.append(f\"❌ Subdirectory '{location}' does not contain a file named 'test_labels.csv'.\")\n",
        "                continue\n",
        "\n",
        "            # Validate the contents of test_labels.csv\n",
        "            test_labels_path = test_labels_files[0]\n",
        "            try:\n",
        "                with zip_ref.open(test_labels_path) as file:\n",
        "                    df = pd.read_csv(TextIOWrapper(file, 'utf-8'), header=None)\n",
        "\n",
        "                # Check header\n",
        "                if list(df.iloc[0]) != required_header:\n",
        "                    report.append(f\"❌ The header in '{test_labels_path}' is incorrect. Expected: {required_header}\")\n",
        "                else:\n",
        "                    report.append(f\"✅ The header in '{test_labels_path}' is correct.\")\n",
        "\n",
        "                # Check column count\n",
        "                if df.shape[1] != 11:\n",
        "                    report.append(f\"❌ '{test_labels_path}' does not have exactly 11 columns.\")\n",
        "                    continue\n",
        "\n",
        "                # Validate first column and other columns\n",
        "                first_column = df.iloc[1:, 0].astype(str)\n",
        "                remaining_columns = df.iloc[1:, 1:].apply(pd.to_numeric, errors='coerce')\n",
        "                if not first_column.str.endswith(\".mp3\").all():\n",
        "                    report.append(f\"❌ The first column of '{test_labels_path}' contains values that are not filenames with an 'mp3' extension.\")\n",
        "                if not ((remaining_columns == 0) | (remaining_columns == 1)).all().all():\n",
        "                    report.append(f\"❌ The non-header columns of '{test_labels_path}' contain values other than 0 or 1.\")\n",
        "                else:\n",
        "                    report.append(f\"✅ The file '{test_labels_path}' meets all column requirements.\")\n",
        "            except Exception as e:\n",
        "                report.append(f\"❌ Error reading or validating '{test_labels_path}': {e}\")\n",
        "\n",
        "    return report\n",
        "\n",
        "validation_report = validate_zip_file(root_folder + submission_name + \".zip\")\n",
        "\n",
        "print(\"\\nValidation Report:\")\n",
        "for line in validation_report:\n",
        "    print(line)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSygqlhNefyk",
        "outputId": "a1a275ad-a148-40c5-aeaa-8ae95ef17471"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation Report:\n",
            "✅ Zip file name format is valid.\n",
            "✅ Top-level directory '2023' exists.\n",
            "✅ All required subdirectories are present.\n",
            "✅ The header in '2023_predictions/BRY/test_labels.csv' is correct.\n",
            "✅ The file '2023_predictions/BRY/test_labels.csv' meets all column requirements.\n",
            "✅ The header in '2023_predictions/CAL/test_labels.csv' is correct.\n",
            "✅ The file '2023_predictions/CAL/test_labels.csv' meets all column requirements.\n",
            "✅ The header in '2023_predictions/FIO/test_labels.csv' is correct.\n",
            "✅ The file '2023_predictions/FIO/test_labels.csv' meets all column requirements.\n",
            "✅ The header in '2023_predictions/HAR/test_labels.csv' is correct.\n",
            "✅ The file '2023_predictions/HAR/test_labels.csv' meets all column requirements.\n",
            "✅ The header in '2023_predictions/KEA/test_labels.csv' is correct.\n",
            "✅ The file '2023_predictions/KEA/test_labels.csv' meets all column requirements.\n",
            "✅ The header in '2023_predictions/LAW/test_labels.csv' is correct.\n",
            "✅ The file '2023_predictions/LAW/test_labels.csv' meets all column requirements.\n",
            "✅ The header in '2023_predictions/LIF/test_labels.csv' is correct.\n",
            "✅ The file '2023_predictions/LIF/test_labels.csv' meets all column requirements.\n",
            "✅ The header in '2023_predictions/MCK/test_labels.csv' is correct.\n",
            "✅ The file '2023_predictions/MCK/test_labels.csv' meets all column requirements.\n",
            "✅ The header in '2023_predictions/PEN/test_labels.csv' is correct.\n",
            "✅ The file '2023_predictions/PEN/test_labels.csv' meets all column requirements.\n",
            "✅ The header in '2023_predictions/SYL/test_labels.csv' is correct.\n",
            "✅ The file '2023_predictions/SYL/test_labels.csv' meets all column requirements.\n",
            "✅ The header in '2023_predictions/WAT/test_labels.csv' is correct.\n",
            "✅ The file '2023_predictions/WAT/test_labels.csv' meets all column requirements.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}